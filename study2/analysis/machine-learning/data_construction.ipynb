{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Construction\n",
    "\n",
    "The single `.csv` file used in the machine learning models for Case Study 2 is created using this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regression Dataset\n",
    "\n",
    "The original data can be found [here](https://zenodo.org/records/8415066). Please download this data locally, then set the `data_dir` variable to point to the directory the data is contained in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = \"raw_data/\"  # DATA/WSD4FEDSRM/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(data_dir + \"Borg data/borg_data.csv\")\n",
    "\n",
    "# Drop all columns of time greater than 90 sec\n",
    "data_temp = data.copy()\n",
    "for col in data_temp.columns:\n",
    "    if '_sec' in str(col) and int(col.split('_')[0]) > 90 or 'end_' in str(col):\n",
    "        data = data.drop(col,axis=1)\n",
    "\n",
    "# Remove all rows containing external rotation task\n",
    "data = data[~data['task_order'].str.endswith('e')]\n",
    "\n",
    "data['subject'] = data['subject'].ffill()  # Replace NA subjects with last seen\n",
    "\n",
    "# Remove all rows of subjects who did not complete full 90 seconds\n",
    "data = data.dropna()\n",
    "\n",
    "# Pivot time columns into one column\n",
    "fatigue_cols = ['before_task', '10_sec', '20_sec', '30_sec', '40_sec', '50_sec', '60_sec', '70_sec', '80_sec', '90_sec']\n",
    "data = pd.melt(data, id_vars=['subject', 'task_order'], value_vars=fatigue_cols, var_name='time', value_name='fatigue')\n",
    "\n",
    "# Make time number instead of string\n",
    "data['time'] = data['time'].str.replace('_sec', '', regex=False)\n",
    "data['time'] = data['time'].replace('before_task', 0.01)\n",
    "data['time'] = data['time'].astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataframe of demographic data\n",
    "demo_anthro = pd.read_csv(data_dir + \"/Demographic and antropometric data/demographic&anthro.csv\")\n",
    "demo_anthro = demo_anthro[['subject', 'age', 'height(cm)', 'dominant_hand', 'sex', 'BMI(kg/m2)']]\n",
    "\n",
    "data = data.merge(demo_anthro, on='subject').sort_values(['subject', 'task_order', 'time'])\n",
    "data.to_csv(\"final_data/data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Subjects:   0%|          | 0/25 [00:00<?, ?it/s]C:\\Users\\Build\\AppData\\Local\\Temp\\ipykernel_57216\\1401845094.py:46: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  sensor_df = pd.concat([sensor_df, new_row], ignore_index=True)\n",
      "Subjects: 100%|██████████| 25/25 [00:18<00:00,  1.35it/s]\n"
     ]
    }
   ],
   "source": [
    "sensor_data_dir = data_dir + \"EMG, IMU, and PPG data/\"\n",
    "\n",
    "# All directory lists created\n",
    "sensor_dir_list = list(filter(lambda x: x.split()[1] == 'internal', list(os.listdir(sensor_data_dir))))\n",
    "subject_list = [s.split('_')[0].title() + \" \" + s.split('_')[1] for s in data['subject'].unique()]\n",
    "sensors = [s.lower() for s in list(os.listdir(sensor_data_dir + sensor_dir_list[0] + '/' + subject_list[0] + '/IMU data')) if s != 'Pelvis']\n",
    "sensor_data_types = ['acc', 'gyr']\n",
    "\n",
    "# Dictionary that contains subjects as keys and list of tasks completed as value\n",
    "subject_tasks_dict = dict(zip(list(data['subject'].sort_values().unique()), list(data.groupby('subject')['task_order'].unique())))\n",
    "\n",
    "\n",
    "\n",
    "# Create dataframe to store sensor data\n",
    "sensor_df_cols = ['subject', 'task_order', 'sensor', 'type', 'time', 'x', 'y', 'z']\n",
    "sensor_df = pd.DataFrame(columns=sensor_df_cols)\n",
    "\n",
    "# Nightmare fuel\n",
    "for subject in tqdm(subject_list, desc=\"Subjects\"):\n",
    "    sensor_dir_list_counter = 0\n",
    "    for d in sensor_dir_list:\n",
    "        sensor_dir_list_counter += 1\n",
    "        if f\"task{sensor_dir_list_counter}_{d[0]}5i\" in subject_tasks_dict[subject.lower().replace(' ', '_')]:\n",
    "            for sensor in sensors:\n",
    "                for t in sensor_data_types:\n",
    "                    # CSV directory created, csv read using this dir\n",
    "                    csv_dir = sensor_data_dir + d + '/' + subject + '/IMU data/' + sensor + '/' + t + '_' + sensor.replace(' ', '_') + '.csv'\n",
    "                    csv_df = pd.read_csv(csv_dir)\n",
    "\n",
    "                    # Get \n",
    "                    length = 10\n",
    "                    x, y, z = csv_df.iloc[::1000, 0].head(length), csv_df.iloc[::1000, 1].head(length), csv_df.iloc[::1000, 2].head(length)\n",
    "\n",
    "                    # Create lists to add to row section to be added\n",
    "                    task_order = [f\"task{sensor_dir_list_counter}_{d[0]}5i\"] * length  # Task list\n",
    "                    subject_lst = [subject.lower().replace(' ', '_')] * length  # Subject list\n",
    "                    sensor_lst = [sensor] * length  # Sensor list\n",
    "                    t_lst = [t] * length  # Type list\n",
    "                    time_lst = [0.01] + list(range(10, 100, 10))  # Time list\n",
    "                    new_row = pd.DataFrame({\n",
    "                        'subject': subject_lst, 'task_order': task_order, 'sensor': sensor_lst, 'type': t_lst, 'time':time_lst,\n",
    "                        'x': x, 'y': y, 'z': z\n",
    "                    })\n",
    "\n",
    "                    # Add row to dataframe\n",
    "                    sensor_df = pd.concat([sensor_df, new_row], ignore_index=True)\n",
    "\n",
    "sensor_df.to_csv(\"final_data/sensor_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge data with sensor data\n",
    "data = data.merge(sensor_df, on=['subject', 'task_order', 'time'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Build\\AppData\\Local\\Temp\\ipykernel_57216\\1472034674.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data['sex'] = data['sex'].replace(to_replace=['male', 'female'], value=[0, 1])\n"
     ]
    }
   ],
   "source": [
    "# Fancy code to turn gyr/acc column into separate x/y/z columns\n",
    "data = data.pivot_table(\n",
    "    index=['subject', 'task_order', 'time', 'fatigue', 'age', 'height(cm)', 'dominant_hand', 'sex', 'BMI(kg/m2)', 'sensor'], \n",
    "    columns='type', values=['x', 'y', 'z']).reset_index()\n",
    "data.columns = ['_'.join(col).strip() if col[1] else col[0] for col in data.columns.values]\n",
    "\n",
    "# Convert sex to 0/1 format\n",
    "data['sex'] = data['sex'].replace(to_replace=['male', 'female'], value=[0, 1])\n",
    "data['dominant_hand'] = data['dominant_hand'].replace(to_replace=['left', 'right'], value=[0, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to CSV\n",
    "data.to_csv('final_data/final_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification Dataset\n",
    "\n",
    "This reformats the data so that the response is in binary format (fatigued/not fatigued). It also retains only the first and last 20% of each subject under each task.\n",
    "\n",
    "Note: Needs the regression dataset first to be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"final_data/final_data.csv\")  # Load regression dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Build\\AppData\\Local\\Temp\\ipykernel_57216\\282939790.py:13: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  result_df = g.apply(get_quantile_rows).reset_index(drop=True)\n"
     ]
    }
   ],
   "source": [
    "def get_quantile_rows(group):\n",
    "    n = len(group)\n",
    "    first = group.iloc[:int(n * 0.2)]\n",
    "    last = group.iloc[int(n * 0.8):]\n",
    "\n",
    "    # Ensures that subject's final fatigue exceeds 14, so that\n",
    "    #   there is theoretically sufficent difference between the \n",
    "    #   non-fatigued and fatigued data.\n",
    "    if max(last['fatigue']) > (20 - 6) / 2:\n",
    "        first['fatigue'] = 0\n",
    "        last['fatigue'] = 1\n",
    "\n",
    "        return pd.concat([first, last])\n",
    "\n",
    "g = data.groupby(['subject', 'task_order'])\n",
    "result_df = g.apply(get_quantile_rows).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_csv(\"final_data/final_data_classif.csv\", index=False)  # Save data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
